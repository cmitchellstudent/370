{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:45:59.618941200Z",
     "start_time": "2024-03-14T16:45:59.615043300Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# MPL block\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1/10 I highly recommend everyone get drunk with friends and watch this     ', \"1/10 My introduction to John cena's absolutely delicious looking biceps     \", '5/10 Not fast enough!     ', '10/10 On march 24th 2023 Kanye West said he is no longer anti semitic because of this movie and Jonah Hill     ', \"4/10 Why are old people having fun they're literally gonna die tomorrow ðŸ˜­     \", '6/10 Kids movie where dolls fight for their lives after the world ends in a terrifying way   somehow still less scary than happy feet     ', '9/10 Yeah my ass was crying again my ass is always crying     ', '6/10 Worst theater experience of my life fun movie tho     ', '9/10 Wow wow wow wow wow WOW     ', '10/10 Severe lack of solar systems but overall a good time     ']\n"
     ]
    }
   ],
   "source": [
    "#big string\n",
    "str = \"\"              \n",
    "df = pd.read_csv('C:\\\\Users\\Connor\\Desktop\\\\370\\letterboxdCorpus\\\\all.csv')\n",
    "df.head()\n",
    "reviews = df.reviews.tolist()\n",
    "print(reviews[0:10])\n",
    "for review in reviews:\n",
    "    str += review[5:] + \" \""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T17:07:56.395106400Z",
     "start_time": "2024-03-14T17:07:09.316722600Z"
    }
   },
   "id": "8e453861643f2906",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def lemmify(a_string):\n",
    "    \"\"\"\n",
    "    processed takes a string and returns a list of lemmas\n",
    "    Requires the following imports:\n",
    "    -------------------------------\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \"\"\"\n",
    "    # Get rid of HTML (or HTML-like) tags\n",
    "    clean = re.sub('<.*?>', '', a_string)\n",
    "    # first we lower-case everything\n",
    "    lowered = clean.lower()\n",
    "    # then tokenize\n",
    "    tokens = word_tokenize(lowered)\n",
    "    # remove stopwords\n",
    "    words = [token for token in tokens if token not in stoplist]\n",
    "    # lemmatize\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Return a list of lemmas\n",
    "    return lemmas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:53:36.781975500Z",
     "start_time": "2024-03-14T16:53:36.780470300Z"
    }
   },
   "id": "fcbed02b4a625ccd",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "podcast sleazoids; full review; bonus eps; review screencrush; feel\n",
      "like; talked full; full alongside; itunes spotify; latest patron-\n",
      "exclusive; 've seen; full discussion; suspense killing; subscribing\n",
      "patreon; every single; gain access; kfbr392 kfbr392; ever seen; free\n",
      "episode; covered far; 've ever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard ( expensive ! ) ( nother ) disposable netflix thriller fails\n",
      "anything potentially clever premise â€” pharmaceutical company basically\n",
      "contrives , along helpless yet eventually complicit military , stage\n",
      "fight kong godzilla boost tv rating network sponsor . `` know rule !\n",
      "'' solitude hideous urban architecture edward hopper roaring engine\n",
      "bullitt . essay , will- 's fine . '' eight crazy night '' took class\n",
      "warfare . , full amusing character , 's 16 minute long gave lupin\n",
      "bigger role movie ca . comedic drama two different movie year ( aside\n",
      "intermittent appreciation brian eno got `` 24\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"hard ( expensive ! ) ( nother ) disposable netflix thriller fails\\nanything potentially clever premise â€” pharmaceutical company basically\\ncontrives , along helpless yet eventually complicit military , stage\\nfight kong godzilla boost tv rating network sponsor . `` know rule !\\n'' solitude hideous urban architecture edward hopper roaring engine\\nbullitt . essay , will- 's fine . '' eight crazy night '' took class\\nwarfare . , full amusing character , 's 16 minute long gave lupin\\nbigger role movie ca . comedic drama two different movie year ( aside\\nintermittent appreciation brian eno got `` 24\""
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokens = lemmify(str)\n",
    "\n",
    "corpus = nltk.Text(tokens)\n",
    "\n",
    "corpus.collocations()\n",
    "corpus.generate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T17:08:54.245878400Z",
     "start_time": "2024-03-14T17:08:38.520092600Z"
    }
   },
   "id": "3fa155e683b51f47",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('axel', 'webber')\n",
      "('webber', 'every')\n",
      "('every', 'night')\n",
      "('night', 'head')\n",
      "('head', \"'s\")\n",
      "(\"'s\", 'hard')\n",
      "('hard', 'find')\n",
      "('find', 'something')\n",
      "('something', 'write')\n",
      "('write', '.')\n",
      "('.', \"'s\")\n",
      "(\"'s\", 'many')\n",
      "('many', 'thing')\n",
      "('thing', 'going')\n",
      "('going', 'wrong')\n",
      "('wrong', 'life')\n",
      "('life', 'right')\n",
      "('right', 'many')\n",
      "('many', 'thing')\n"
     ]
    }
   ],
   "source": [
    "bigrams = nltk.bigrams(tokens[100:120])\n",
    "\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:59:15.333197Z",
     "start_time": "2024-03-14T16:59:15.329688400Z"
    }
   },
   "id": "7680b186c51ec01f",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m vec \u001B[38;5;241m=\u001B[39m CountVectorizer(preprocessor \u001B[38;5;241m=\u001B[39m lemmify, ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m3\u001B[39m))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# matrix of ngrams\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m ngrams \u001B[38;5;241m=\u001B[39m vec\u001B[38;5;241m.\u001B[39mfit_transform(reviews)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# count frequency of ngrams\u001B[39;00m\n\u001B[0;32m      8\u001B[0m count_values \u001B[38;5;241m=\u001B[39m ngrams\u001B[38;5;241m.\u001B[39mtoarray()\u001B[38;5;241m.\u001B[39msum(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\370\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1380\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1381\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1382\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1383\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1384\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1385\u001B[0m             )\n\u001B[0;32m   1386\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1388\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_count_vocab(raw_documents, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfixed_vocabulary_)\n\u001B[0;32m   1390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1391\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\370\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1273\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1274\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1275\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m analyze(doc):\n\u001B[0;32m   1276\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1277\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\370\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    111\u001B[0m     doc \u001B[38;5;241m=\u001B[39m preprocessor(doc)\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 113\u001B[0m     doc \u001B[38;5;241m=\u001B[39m tokenizer(doc)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ngrams \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stop_words \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "# Say hello to our old friend:\n",
    "vec = CountVectorizer(preprocessor = lemmify, ngram_range=(2,3))\n",
    "\n",
    "# matrix of ngrams\n",
    "ngrams = vec.fit_transform(reviews)\n",
    "\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "\n",
    "# list of ngrams\n",
    "vocab = vec.vocabulary_\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()],\n",
    "                            reverse=True)).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
    "df_ngram.shape\n",
    "df_ngram[20:40]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T17:07:56.430282Z",
     "start_time": "2024-03-14T17:07:56.398109700Z"
    }
   },
   "id": "7ff35a9fffafcb64",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c643fe07e086510"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
