{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Sentiment Analysis \n",
    "these first few blocks are me copy-pasting the https://realpython.com/python-nltk-sentiment-analysis/#using-nltks-pre-trained-sentiment-analyzer \n",
    "code and learning how it works"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10a00a8c0979664"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:22:45.127177500Z",
     "start_time": "2024-04-09T04:22:43.579858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:22:46.998803300Z",
     "start_time": "2024-04-09T04:22:45.129179400Z"
    }
   },
   "id": "22da11d57ed0e665",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> False RT @TelePolitics: BBC Question Time: Ed Miliband savaged over his economic record http//t.co/cToppiRYPI\n",
      "> True In the Porsche paddock at #BudapestGP and I found out #JohnyHerbert from @SkySportsF1 is rolling with us this weekend! :) #PorscheSuperCup\n",
      "> False RT @davidcbocko: @MichaelLCrick If you're Nick Robinson apparently your duty is to act as a spokesperson for Tory central office.\n",
      "> True RT @joncstone: ICM snap poll for the #bbcqt special has a solid win for David Cameron: \n",
      "\n",
      "44% Cameron\n",
      "38% Miliband\n",
      "19% Clegg http//t.co/C3G…\n",
      "> True @BeeGeeBeard @johnfm15 @OwenJones84 Why are you so keen to work with these \"Red Tories\" in government?\n",
      "> True Appreciate your recent Retweets ! @montblancforum @Les_Scop_idf @MiroirSocial @thomasdeysieux @EMauFouX Have a great Friday :)\n",
      "> True RT @BartchJamie: Amazing how quiet labour get when ed Miliband is on the telly you'd think they were avoiding drawing attention to him,\n",
      "> True @iamgiant Hope Ryan washed his hair later. Was great night and excellent music at Refuel Dunedin last night :D http//t.co/t7gX9k1c59\n",
      "> True RT @carlmaxim: We should have special rooms set aside in pubs for people to express UKIP views without imposing them on anyone else.\n",
      "#AskNi…\n",
      "> False SNP leader faces audience questions http//t.co/TYClKltSpW\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:22:47.019325700Z",
     "start_time": "2024-04-09T04:22:47.000805900Z"
    }
   },
   "id": "a43edf21a1ca9997",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## my turn\n",
    "Doing the same as above, but now with my corpus instead of tweets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9c9f8be190822bb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I highly recommend everyone get drunk with friends and watch this     ', \"My introduction to John cena's absolutely delicious looking biceps     \", 'Not fast enough!     ', ' On march 24th 2023 Kanye West said he is no longer anti semitic because of this movie and Jonah Hill     ', \"Why are old people having fun they're literally gonna die tomorrow 😭     \", 'Kids movie where dolls fight for their lives after the world ends in a terrifying way   somehow still less scary than happy feet     ', 'Yeah my ass was crying again my ass is always crying     ', 'Worst theater experience of my life fun movie tho     ', 'Wow wow wow wow wow WOW     ', ' Severe lack of solar systems but overall a good time     ']\n"
     ]
    }
   ],
   "source": [
    "#corpus as list of strings\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:\\\\Users\\Connor\\Desktop\\\\370\\letterboxdCorpus\\\\all.csv')\n",
    "reviews = df.reviews.tolist()\n",
    "scoreless = []\n",
    "for review in reviews:\n",
    "    scoreless.append(review[5:])\n",
    "print(scoreless[0:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:22:47.495722700Z",
     "start_time": "2024-04-09T04:22:47.017323400Z"
    }
   },
   "id": "84a9143414323b78",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> True Two thoughts:  1) This is an incredible performance by McConaughey, and one that makes smart use of his existing star text in new, interesting ways; Wooderson gone to seed. Jared Leto is excellent as well.  2) These incredible performances are in a movie that lives into every negative stereotype of Hollywood filmmakers examining a minority issue--in this case the impact of the HIV virus on the gay community in the 1980s, and the FDA's lethargic response to it--from the perspective of a heroic straight white dude. It's not necessarily a bad version of that kind of movie. But it's still that kind of movie.     \n",
      "> False \"We'll have a dalmatian plantation!\" or \"Kill two for matching clogs!\"  So much weird texture here, all bound up in those gorgeous Xeroxed inks, everything has a dusty coating of toner. And what's with the vague anti-capitalist vibe? There's a whole middle section here that takes place in this decrepit mansion, sort of a crumbling bourgeois symbol ruled over by Cruella, a vicious one-percenter who lives only to consume conspicuously. While the puppies escape, two henchmen watch a bizarre game...     \n",
      "> True Sheer commitment to the sugary bit takes this baby movie a bit further than it probably should. It's pretty funny, nice old-school forced perspective and production design/animation at the north pole, I like hanging with my pals Zooey D and Jimmy C.     \n",
      "> False \"Over there in China, they's all wantin' to eat macaroni and cheese. Don't you think that's kinda odd, what with all the Chinese food they got?\"  Nicoletta Braschi is carrying around a copy of Orlando furioso, a sprawling epic poem in which (among a great many other things) a warrior relentlessly pursues a possibly insane princess who does not love him, only to learn that love is in itself a kind of madness. this is another of Jarmusch's triptychs of...     \n",
      "> False ly made it through this, one of the worst movies I've ever seen. An ultra-cheap amateur vanity project for the director's terrifying wife, 125 inexcusable minutes of vaguely giallo-ish tedium starring the guy from SGT. KABUKIMAN and Donald Pleasance in his final screen role.  Discussed on Episode 30 of The Suspense is Killing Us.     \n",
      "> True I no longer like movies.     \n",
      "> False F FOR FULCI     \n",
      "> False when lucas hedges cry i cry that's just how it works     \n",
      "> False GOTTI 2: CRUISE CONTROL     \n",
      "> True it had its epic nolan moments but also feels like it wastes a good bit of its own runtime     \n"
     ]
    }
   ],
   "source": [
    "shuffle(scoreless)\n",
    "for review in scoreless[:10]:\n",
    "    print(\">\", is_positive(review), review)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:22:47.513738400Z",
     "start_time": "2024-04-09T04:22:47.493721100Z"
    }
   },
   "id": "19a961be0aa6f45e",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "I honestly don't mind the supposed ~60% accuracy. I do like the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bfb09064667cc06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generating top 100s"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da607beacd76729b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "#\n",
    "#here's my own injection, which makes a list of words in positive reviews and a list\n",
    "#of negative reviews, so that I don't need to use the nltk.corpus movie reviews\n",
    "pos_category = []\n",
    "neg_category = []\n",
    "for review in scoreless:\n",
    "    words = review.split()\n",
    "    if is_positive(review): \n",
    "        for word in words:\n",
    "            pos_category.append(word)\n",
    "    else:\n",
    "        for word in words:\n",
    "            neg_category.append(word)\n",
    "\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#here's where it trains off my stuff rather than nltk movie reviews\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(pos_category)\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(neg_category)\n",
    ")]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:24:06.360209800Z",
     "start_time": "2024-04-09T04:22:47.514739400Z"
    }
   },
   "id": "9da16cc4dbf8dcdf",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSTIVE:\n",
      "{'satisfied', 'owning', 'revered', 'disappoint', 'prominent', 'thankful', 'kissing', 'joyful', 'easiest', 'inspirational', 'subscribing', 'cutest', 'Subscribing', 'beguiling', 'delighted', 'breathlessly', 'hopeful', 'owed', 'costuming', 'intelligent', 'thrilled', 'approach', 'supportive', 'mined', 'curly', 'exacting', 'superb', 'sharply', 'lovelorn', 'obscure', 'pls', 'ground', 'inclusive', 'hong', 'saturday', 'folks', 'Besides', 'subverts', 'pleasantly', 'remained', 'richly', 'celebrate', 'gimmick', 'impoverished', 'earth', 'blessed', 'minding', 'emerges', 'Easily', 'improvisational', 'entered', 'united', 'documented', 'WANT', 'rebellious', 'kidding', 'unafraid', 'uncharted', 'smile', 'extend', 'perceive', 'hagiographic', 'eps', 'gain', 'transcends', 'grateful', 'cringe', 'panoramic', 'libertarian', 'vibe', 'smooth', 'luckily', 'mandatory', 'seamless', 'Seems', 'Preparing', 'underwhelming', 'enthusiastic', 'captivated', 'sporadically', 'subconsciously', 'award', 'sugary', 'intricately', 'credits', 'maniacal', 'bat', 'euphoric', 'breezy', 'tweeted', 'Best', 'automatic', 'microcosmic', 'undervalued', 'pleasing', 'flip', 'ranked', 'eagerly', 'treasure', 'ecstatically'}\n",
      "NEGATIVE:\n",
      "{'uh', 'scumbag', 'deakins', 'lock', 'illuminates', 'Along', 'springs', 'smug', 'plan', 'bury', 'choke', 'repulsed', 'posed', 'torture', 'anguish', 'assaulted', 'farcically', 'fatally', 'petrified', 'jingoistic', 'willingly', 'underage', 'answers', 'harvest', 'scathing', 'arrhythmic', 'animalistic', 'confounding', 'blindly', 'stave', 'freewheeling', 'confronts', 'nope', 'timothée', 'Appropriately', 'flagrantly', 'excruciatingly', 'shoving', 'earning', 'identifies', 'mangled', 'polarizing', 'bitch', 'pickup', 'tighten', 'revisionist', 'bafflingly', 'patently', 'fires', 'armiesToo', 'dehumanizing', 'tryna', 'exhilaratingly', 'violates', 'sturdily', 'amnesiac', 'Viewed', 'performs', 'punish', 'bums', 'placid', 'lousy', 'ooky', 'buys', 'suicidal', 'artless', 'gaslight', 'pioneering', 'meaningless', 'disfigured', 'brutalize', 'stream', 'trembling', 'mourn', 'crawl', 'hairy', 'systematic', 'opting', 'external', 'Classic', 'bitterly', 'assaultive', 'Violent', 'juxtaposed', 'sorrowful', 'reverberating', 'discussed', 'collapses', 'smoked', 'unmotivated', 'hunted', 'reported', 'abducted', 'respond', 'sift', 'regrettably', 'uptight', 'scarred', 'Basically', 'leering'}\n"
     ]
    }
   ],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}\n",
    "print(\"POSTIVE:\")\n",
    "print(top_100_positive)\n",
    "print(\"NEGATIVE:\")\n",
    "print(top_100_negative)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:24:06.493290300Z",
     "start_time": "2024-04-09T04:24:06.363212800Z"
    }
   },
   "id": "f1254a2eb71ff700",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, there are the rough top 100 positive and negative words in my corpus. Apart from a few questionable results here and there, I'd consider it mostly good."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edc27ef32a459070"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's an idea. I'm going to redefine my pos/neg_category variables using the user scores attached to reviews. I'm hoping it gives me more accurate evaluations than the polarity scores"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8860e410e602db4e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'highly', 'recommend', 'everyone', 'get', 'drunk', 'with', 'friends', 'and', 'watch', 'this', 'my', 'introduction', 'to', 'john', 'cena', \"'s\", 'absolutely', 'delicious', 'looking', 'biceps', 'i', 'like', 'that', 'one', 'robert', 'de', 'niro', 'comedy', 'where', 'he', 'shows', 'his', 'cock', 'and', 'fucks', 'like', 'crazy', 'this', 'is', 'not', 'that', 'one', 'tho', 'i', 'know', 'a', 'guy', 'named', 'aksel']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#8s, 9s, and 10/10s\n",
    "good_movies = []\n",
    "#3s, 2s, and 1/10s\n",
    "bad_movies = []\n",
    "#var for later\n",
    "sentences = []\n",
    "for review in reviews:\n",
    "    if review.startswith(\"10\") or review.startswith(\"9\") or review.startswith(\"8\"):\n",
    "        lower = review[5:].lower()\n",
    "        tokens = word_tokenize(lower)\n",
    "        for token in tokens:\n",
    "            good_movies.append(token)\n",
    "    if review.startswith(\"1/\") or review.startswith(\"2\") or review.startswith(\"3\"):\n",
    "        lower = review[5:].lower()\n",
    "        tokens = word_tokenize(lower)\n",
    "        for token in tokens:\n",
    "            bad_movies.append(token)\n",
    "    lower2 = review[5:].lower()\n",
    "    tokens2 = word_tokenize(lower2)\n",
    "    sentences.append(tokens2)\n",
    "print(bad_movies[0:50])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:24:14.669520900Z",
     "start_time": "2024-04-09T04:24:06.493290300Z"
    }
   },
   "id": "dce4b857be449232",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'm too lazy to go up and make a function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b65c9f8aac0718"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSTIVE:\n",
      "{'allows', 'bodily', 'subscribing', 'gothic', 'inherent', 'grounded', 'vivid', 'detailed', 'stronger', 'scariest', 'warm', 'devastating', 'empathetic', 'tiny', 'noir', 'moves', 'masterful', 'vital', 'sudden', 'ep', 'fantastical', 'adds', 'trapped', 'rewatch', 'pulls', 'grew', 'shaking', 'gleefully', 'tense', 'internal', 'sees', 'wrapped', 'neon', 'delicate', 'exquisite', 'twin', 'sensual', 'familial', 'experimental', 'atmosphere', 'marvelous', 'loving', 'elegant', 'gorgeously', 'builds', 'detective', 'apocalyptic', 'melancholic', 'lying', 'expertly', 'harsh', 'layered', 'harrowing', 'claustrophobic', 'flawed', 'grateful', 'ghostly', 'phenomenal', 'indelible', 'fitting', 'finds', 'hysterical', 'showcasing', 'unexpected', 'surreal', 'uses', 'magnificent', 'underrated', 'breaking', 'nightmarish', 'shown', 'similar', 'touching', 'essential', 'dreamy', 'vividly', 'classical', 'gentle', 'allow', 'adding', 'fractured', 'vicious', 'beautifully', 'flows', 'existential', 'impeccable', 'terrified', 'subtle', 'operating', 'hearing', 'revolutionary', 'tight', 'elemental', 'experience', 'thrilling', 'captures', 'nonetheless', 'abstract', 'intimate', 'invented'}\n",
      "NEGATIVE:\n",
      "{'telegraphed', 'wringing', 'interchangeable', 'smug', 'rank', 'extraneous', 'hbo', 'elephant', 'dinesh', 'bedazzled', 'cw', 'aspires', 'guessing', 'unmitigated', 'counterfeit', 'redeemed', 'inconceivable', 'discernible', 'trolling', 'lucrative', 'tattoo', 'benign', 'unsolicited', 'insipid', 'fatally', 'dilutes', 'willingly', 'legs', 'cutesy', 'upgrade', 'inert', 'falsely', 'flatulent', 'cribbed', 'asexual', 'neeson', 'limp', 'pains', 'factual', 'steaming', 'metric', 'asinine', 'flavorless', 'forehead', 'paired', 'mistakes', 'digitally', 'resurrect', 'oppressively', 'foaming', 'downhill', 'regurgitating', 'fuckahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhfuck', 'haphazardly', 'apocryphal', 'prosthetic', 'kooky', 'squanders', 'artless', 'crap', 'recycling', 'artemis', 'japan', 'contend', 'conspicuous', 'misleading', 'unexciting', 'bland', 'laziest', 'barring', 'orientalist', 'comped', 'wastes', 'adequate', 'prevent', 'glorified', 'smugly', 'careless', 'faintest', 'greenscreen', 'abysmal', 'phenomenally', 'vacuous', 'ass', 'aping', 'insufferably', 'gong', 'yall', 'wadlow', 'weakened', 'twist', 'finance', 'lifeless', 'clueless', 'tarnish', 'ought', 'slapdash', 'redefine', 'lame', 'contrary'}\n"
     ]
    }
   ],
   "source": [
    "#now processing new stuff see line 4 and 8\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(good_movies)\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(bad_movies)\n",
    ")]\n",
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}\n",
    "print(\"POSTIVE:\")\n",
    "print(top_100_positive)\n",
    "print(\"NEGATIVE:\")\n",
    "print(top_100_negative)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:24:49.390755600Z",
     "start_time": "2024-04-09T04:24:14.669520900Z"
    }
   },
   "id": "38ab2b350803b620",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even cooler results. Although maybe affected by a smaller sample size, since I'm seeing the infrequently used \"fuckahhhhhhhhh\" in the negative.\n",
    "\n",
    "So my best idea to find more stuff with word embeddings is to process my positive and negative words and see what pops up frequently in .most_similar calls"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d7f57c52535d10d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25344\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "print(len(sentences))\n",
    "model = Word2Vec(sentences = sentences,\n",
    "                 window = 2, \n",
    "                 min_count =1,\n",
    "                 vector_size = 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:24:51.651472600Z",
     "start_time": "2024-04-09T04:24:49.387752900Z"
    }
   },
   "id": "91956735a39ce755",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS \"SIMILAR TO\" POSITIVE WORDS:\n",
      "natural:32  throwback:31  avatar:30  pulp:30  fits:30  lo-fi:30  petty:30  anonymous:29  thrills:29  blurring:29  dtv:29  zac:28  cavities:28  dangerous:28  graphic:28  purity:27  conventions:27  ambitions:27  gruesome:27  zooms:27  incoherent:27  paramedics:27  videos:27  promises:27  awkward:27  famous:27  chase:27  caper:27  styles:26  elite:26  exercise:26  vintage:26  kaiju:26  effect:26  heist:26  sincere:26  stature:26  pageantry:26  grisly:26  conspiracy:25  network:25  grand:25  empathetic:25  high-concept:25  chamber:25  vivid:25  post-war:25  worthy:25  exposition:25  barren:25  class-divided:25  victims:25  meeting:25  relative:25  bedroom:24  coda:24  superficiality:24  expression:24  patience:24  cartoon:24  reformed:24  thematic:24  institutionalized:24  architecture:24  becoming:24  interview:24  erotic:24  infinite:24  biker:24  slipping:24  french:24  affair:24  destruction:24  random:24  canvas:24  gentle:24  staggering:24  ex-soviet:24  protagonist:24  drug:24  raw:24  potentially:24  classics:24  tonally:24  blunt:24  maurier:24  soft:24  recorded:24  convoluted:24  using:24  sports:24  presence:23  capturing:23  detached:23  nastier:23  climactic:23  comically:23  cozy:23  sustained:23  stunt:23  regional:23  sweaty:23  popular:23  lick:23  interviews:23  intermittently:23  setting:23  eminent:23  perverse:23  quote:23  relentless:23  everyman:23  student:23  imminent:23  shock:23  haunting:23  brisk:23  critical:23  potent:23  weakest:22  70s:22  chest:22  quirky:22  expressive:22  showcasing:22  neo-noir:22  terence:22  magazine:22  lion:22  amateur:22  banderas:22  chad:22  trio:22  surfaces:22  twisted:22  gaspar:22  unexpected:22  british:22  nostalgic:22  sentimental:22  fractured:22  cop:22  hop:22  subtly:22  vicious:22  respects:22  flashback:22  occasional:21  extreme:21  severe:21  coming-of-age:21  criminal:21  gory:21  meditative:21  claustrophobic:21  inventive:21  lacked:21  deliberate:21  landscapes:21  evolving:21  poetic:21  afternoon:21  unrepentant:21  universal:21  con:21  messy:21  price:21  combo:21  directors:21  choreography:21  hand-crafted:21  zo:21  delivery:21  machine:21  understanding:21  launch:21  personality:21  hitman-turned-preacher:21  statement:21  detailed:21  lab:21  wise:21  ambivalent:21  makeup:21  dazzling:21  90s:21  detective:21  feature-length:21  mutually:21  shining:21  generous:21  nuclear:20  kicks:20  gang:20  category:20  contributing:20  laced:20  manic:20  technical:20  courtroom:20  \n"
     ]
    }
   ],
   "source": [
    "#similar_words = model.wv.most_similar('sensual', topn= 10)\n",
    "# ^ list of tuples. first word = [0][0]\n",
    "#making a dictionary in the (word, count) format\n",
    "near_positive = {}\n",
    "for word in top_100_positive:\n",
    "    sim = model.wv.most_similar(word, topn= 500)\n",
    "    for i in range(500):\n",
    "        synonym = sim[i][0]\n",
    "        #print(word)\n",
    "        if synonym in near_positive:\n",
    "            near_positive[synonym] += 1\n",
    "        else:\n",
    "            near_positive[synonym] = 1\n",
    "sorted_pos = {k: v for k, v in sorted(near_positive.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(\"WORDS \\\"SIMILAR TO\\\" POSITIVE WORDS:\")\n",
    "message = \"\"\n",
    "for key, value in list(sorted_pos.items())[:200]:\n",
    "    message += f\"{key}:{value}  \"\n",
    "print(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:30:58.754810400Z",
     "start_time": "2024-04-09T04:30:58.669477600Z"
    }
   },
   "id": "8fe4ef81a4fc45d2",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS \"SIMILAR TO\" NEGATIVE WORDS:\n",
      "slow-motion:10  hardcore:10  ride:10  utterly:10  dirty:9  microcosm:9  unearths:9  geography:9  introduce:9  demonstrably:9  slow:9  ragged:9  valuable:9  spy:9  vibe:9  little-seen:9  cosplay:9  derivative:9  confident:9  overlong:9  corny:9  standout:9  half-quizzical:9  exhausting:9  issue:9  endlessly:9  irony:9  cliche:8  gore:8  oblivious:8  atmospheric:8  mines:8  zach:8  featuring:8  substituting:8  buñuel:8  demon:8  nails:8  feminist:8  pretentious:8  following:8  unnerving:8  lacking:8  ugly:8  seriousness:8  annoying:8  signed:8  demand:8  grasp:8  odd:8  mihai:8  unsurprisingly:8  admirable:8  well-meaning:8  co-opted:8  brando:8  realistic:8  slicker:8  motive:8  winking:8  enlightening:8  d'abo:8  unsettled:8  adorable:8  paced:8  artist:8  jettisons:8  bud:8  assistant:8  intertwined:8  dangerous:8  heavily:8  rich:8  operate:8  vital:8  crafted:8  playing:8  executed:8  monotonous:8  racist:8  measure:8  falls:8  needlessly:8  quiet:8  saving:8  adr:8  jot:8  morally:8  cap:8  scumbag:8  gem:8  tantalizing:8  clean:8  remake:8  trilogy:8  twisted:8  framed:8  hollow:8  hauer:8  big-budget:8  stellar:8  shenanigans:8  vehicle:8  homage:8  increasingly:8  bold:8  narratively:8  downright:8  gags:8  hand-crafted:8  thoughtful:8  genius:8  unequivocally:7  audible:7  flower:7  speaking:7  hervé:7  fassbender:7  thailand:7  shark:7  acid:7  festival:7  technological:7  accent:7  mcdonagh:7  shootouts:7  intelligent:7  fx:7  references:7  reductively:7  vulnerable:7  miller:7  cave:7  cassettetapes:7  wet:7  11,000:7  bell:7  sprawling:7  swinging:7  metaphysical:7  relentlessly:7  sets:7  loosely:7  july:7  groups:7  hou:7  area:7  penis:7  wildly:7  tactile:7  illusion:7  wanda:7  flick:7  pulse:7  hyper-accomplished:7  👢:7  presenting:7  remarkable:7  thin:7  spectacular:7  middlebrow:7  dna:7  cuts:7  heartbreaking:7  stylistically:7  repetitive:7  flat:7  conservative:7  accents:7  pit:7  response:7  traumatized:7  53:7  stretched:7  grass:7  propaganda:7  mumblecore:7  faint:7  criticisms:7  catastrophe:7  pipe:7  rodney:7  struggled:7  janelle:7  courting:7  hoult:7  rationalize:7  locke:7  improper:7  wanting:7  half-heartedly:7  explicitly:7  liamkelsall:7  sludge:7  ｈａｖｅ:7  weinsteins:7  flowery:7  typically:7  match:7  perfume:7  \n"
     ]
    }
   ],
   "source": [
    "near_negative = {}\n",
    "for word in top_100_negative:\n",
    "    sim = model.wv.most_similar(word, topn= 500)\n",
    "    for i in range(500):\n",
    "        synonym = sim[i][0]\n",
    "        #print(word)\n",
    "        if synonym in near_negative:\n",
    "            near_negative[synonym] += 1\n",
    "        else:\n",
    "            near_negative[synonym] = 1\n",
    "sorted_neg = {k: v for k, v in sorted(near_negative.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(\"WORDS \\\"SIMILAR TO\\\" NEGATIVE WORDS:\")\n",
    "message = \"\"\n",
    "for key, value in list(sorted_neg.items())[:200]:\n",
    "    message += f\"{key}:{value}  \"\n",
    "print(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:31:31.417834300Z",
     "start_time": "2024-04-09T04:31:31.321425400Z"
    }
   },
   "id": "d52a46cda60b9342",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "These results kinda suck, but improve some as i turn up the topn parameter. I imagine as the count values decrease, the less reliable the words get."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf8ba996781f2323"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4854d1fb7a5b483f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
